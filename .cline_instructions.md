# Development Environment & Guidelines

## System Environment
- **OS**: Windows with PowerShell (NOT Linux/bash commands)
- **Python**: Use venv environments exclusively
- **Shell**: Always use PowerShell syntax for commands
- **File paths**: Use Windows path separators (backslashes or forward slashes)
- **GPU**: NVIDIA GeForce RTX 3070 Ti

## Current Project: XRF_Data_Manager
- **Purpose**: Use to create tables of XRF results for reports and keeps track of metadata: the instrument used, sample type, project, and instrument operator, date. Configuration files are used to update drop down lists of metadata choices. In the end, the XRF results in a project are saved to a concatonated .csv file, and formated tables are saved to an Excel file. - **Details**: If the sample is a standard pressed pellet, the results are presented as absolute concentrations, other wise relative concentraions. Elements reported with unit % are major, with ppm are trace. All generated tables are cross tables with each line representing an element (in order of atomic number), and each column are the results for a sample. The results are summed to 100 for relative concentrations, and for absolute, the bottom lines are balance and sum to 100%. If selected, the results can be represented as common oxides by calculating a factor. Reporting Absolute concentrations requires that the sample be a standard pellet. If elements are ignored or if absolute concentrations need to be presented as relative, the values of the reported concentrations need to be normalized to 100%. If element data is missing from a sample (not detected), don't leave cell blank, instead display "---" or "na" (make this an option). 
- **Key data files**: Results are saved as text files with .qan. (see folder "Example Data"). They have the following structure: if line starts with "S", the next thing is the sample ID then some value for the date (ignore this). If line starts with "D" ignore. For each line starting with "C", the next field is the omnian scan and element used. So Ti4 is titanium omnian 4. Na is sodium omnian 0. Then the concentration, the unit, then the element, the the signal and finally some number refered to as the factor (ignore this). Silver is usually in units of kcps (kilocounts per second) but could also be present in ppm or % if siliver is chose to be quantified by the sepctrometer. 
- **Current status**: Just starting
- **Use Story**: The user has collected .qan and other files from the Purdue lab with the XRF instrument. These are stored in a project folder that starts with a number (the project number). The user directes the XRF_Data_Manager to the folder and all .qan files are automatically detected. If no metadata is available, the program asks the user to provide this data: todays Date, Project number (if possible, read from parent folder?), Client Name (again, go up to the level of client and read folder name if possible?), Operator (use list in config file, start with "Blaise Mibeck", "Anthony Back", "Logan Miles", "Gary George"), Instrument (use list in config file, start with "Purdue PanAlytical Epsilon 4", ), sample type (use list in config file, "standard pellet", "non-standard pellet", "loose powder", "liquid", "coupon", "object"). When finished, save the meta data.  The qan file names should be the sample IDs. If there is no lookup table present, ask the user to fill in a look up table with three columns: Sample IDs, notebook ID, Client ID, Client Name for reporting. When finished save the look up table. Then allow the user to generate tables. Give the user these options: ignore Ag, Sn, Sb and Te (tube elements). Option to report as element or oxide, major or trace, absolute or relative. When a table is generated, give the used the option to save a formated version as a tab in the excel book, also give user to save concatonated table of all data from all samples to a .csv. The concatonated file should be a flat table. Each line is for each measured element. The columns are: Line, Sample ID, Notebook ID, Client ID, Report Abreviation, Z,	Element	Concentration	Unit	Wt.%	Omnian	Oxide	OxideConc.wt%". Most of these come from the .qan file, but several (Wt.%, Oxide, OxideConc.wt%) are calculated, or come from metadata or the look up table. Tables saved to excel are formated the following way: Ariel, 10pt font, top line is bold and has a light grey background, single thin line borders for all cells, First column is Z, then element, then one column for each sample (use abreviated name), after the last element row, If this is major elements, convert the total of trace elements to Wt.% and list as trace, if the results are absolute concentration the next row is the balance (100 - sum). The last row is the total and is the sum of the listed elements, trace elements, and balance. If the table is for absolute or relateive trace elements, list all trace elements in units of ppm. If the the table is oxide, list the calculated oxide wt. %. If aboslute, also list trace oxide and balance. Report all values to the nearest 0.01 Wt.% or nearest 10 ppm. Above the excel table include a draft caption explaining what the table represents. Title the excel tabs with what the table on it is (Absolute Major, Relative Oxide, etc... )

## Preferred Libraries & Stack
- **GUI**: QtPy (preferred over tkinter), use PandasGUI for table preview display
- **Data**: Pandas, NumPy, TensorFlow, SAM, PyTorch
- **Science**: SciPy, scikit-learn
- **Visualization**: Plotly (interactive), Matplotlib (static)
- **Image**: OpenCV, PIL/Pillow, scikit-image
- **Performance**: Numba (for computational intensity), CUDA, 

## Code Organization Rules
- **Architecture Pattern**: Model-View-Controller
- **File size**: Keep Python files under 250 lines, split at 300+ lines
- **Responsibility**: One main purpose per file
- **Structure**: Prefer multiple small files over monolithic files
- **Naming**: Use descriptive filenames that indicate purpose
- **Always suggest refactoring** when files become too large

## Project Structure Guidelines
```
project_root/
├── main.py              # Entry point, orchestration only
├── config.py            # Constants, settings, paths
├── data/                # Data files directory
├── src/                 # Source code modules
│   ├── data_loader.py   # Data import/validation
│   ├── processors.py    # Data cleaning/transformation  
│   ├── analyzers.py     # Analysis algorithms
│   └── visualizers.py   # Plotting/display functions
├── tests/               # Unit tests
└── requirements.txt     # Dependencies
```

## Development Practices
- **Incremental development**: Build and test one component at a time
- **Error handling**: Include try/except blocks for file I/O and data operations
- **Documentation**: Add docstrings to all functions
- **Testing**: Create simple test cases for each module
- **Virtual environment**: Always activate venv before installing packages or running code

## Performance Considerations
- **Large datasets**: Use chunking for files >100MB
- **Computationally intensive**: Consider multiprocessing or Numba
- **Memory**: Monitor memory usage for image/data processing
- **Progress indicators**: Use tqdm for long-running operations

## Common Windows/PowerShell Commands
```powershell
# Virtual environment
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install package_name

# File operations
Get-ChildItem (not ls)
Test-Path (not test)
```

## Error Prevention
- **Always check** if files/directories exist before operations
- **Validate data types** before processing
- **Handle missing values** explicitly in data operations
- **Use relative paths** from project root when possible

## Cost Optimization
- **Focused requests**: Ask for one specific feature at a time
- **Clear context**: Provide current state before requesting changes
- **Avoid repetition**: Don't re-read large files unnecessarily
- **Modular approach**: Work on isolated components to minimize context